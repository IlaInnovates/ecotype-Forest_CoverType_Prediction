# -*- coding: utf-8 -*-
"""ecoType_Forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oP3dHyx9StB0tbsjf3yoOp89I-0Jvcr0

## 1Ô∏è‚É£ Data Collection
"""

import pandas as pd

# 1. Download and load the complete Forest Cover Type dataset using Pandas [cite: 30]
# Note: Ensure the dataset file is in your working directory
try:
    df = pd.read_csv('cover_type.csv')
    print("Dataset loaded successfully.")
except FileNotFoundError:
    print("Error: The file 'cover_type.csv' was not found.")

# 2. Understand the dataset's structure [cite: 31]
print("\n--- Dataset Shape ---")
print(f"Total Rows: {df.shape[0]}") # Expected: 1,45,891 [cite: 17]
print(f"Total Columns: {df.shape[1]}") # Expected: 55 or 13 depending on encoding [cite: 17, 19]

# 3. View column descriptions and initial data [cite: 31]
print("\n--- Column Names ---")
print(df.columns.tolist())

# 4. Identify Target Classes [cite: 31]
print("\n--- Target Classes (Cover_Type) ---")
# The target variable 'Cover_Type' contains 7 classes [cite: 18]
print(df['Cover_Type'].unique())
print(f"Number of unique classes: {df['Cover_Type'].nunique()}")

"""## 2Ô∏è‚É£ Data Understanding"""

import pandas as pd

# 1. Explore dataset structure
print("--- Dataset Shape ---")
print(df.shape)  # Expected: (145891, 55) or similar [cite: 17, 33]

print("\n--- Dataset Information (Data Types & Memory) ---")
df.info()  # Shows column names, non-null counts, and data types

# 2. Statistical Analysis
print("\n--- Statistical Summary of Numerical Features ---")
# This provides mean, std, min, max, and quartiles for features like Elevation and Slope [cite: 21, 33]
print(df.describe())

# 3. Data Quality Checks [cite: 34]
print("\n--- Missing Values Count ---")
# Check for any null values that require imputation in Step 3 [cite: 34, 38]
print(df.isnull().sum())

print("\n--- Duplicate Records ---")
# Identify if there are identical rows that should be removed [cite: 34]
print(f"Total Duplicate Rows: {df.duplicated().sum()}")

# 4. Class Distribution Analysis [cite: 34, 48]
print("\n--- Class Distribution (Target: Cover_Type) ---")
# Identify class imbalance which will be handled later using RandomOverSampler [cite: 34, 51]
print(df['Cover_Type'].value_counts())

# Percentage distribution for better understanding of imbalance [cite: 48]
print("\n--- Class Distribution Percentage ---")
print(df['Cover_Type'].value_counts(normalize=True) * 100)

"""## 3Ô∏è‚É£ Data Cleaning & Transformation"""

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer

# 1. Handle Missing Values using SimpleImputer
# We use the 'median' strategy as it is robust to outliers, which are common in environmental data.
imputer = SimpleImputer(strategy='median')

# Select numerical columns for imputation (excluding any categorical ones if they exist)
numeric_cols = df.select_dtypes(include=[np.number]).columns
df[numeric_cols] = imputer.fit_transform(df[numeric_cols])

print("Missing values after imputation:", df.isnull().sum().sum())

# 2. Fix Skewness in Continuous Variables
# Applying log1p (log(1+x)) transformation to features that often exhibit right-skewness.
# This helps the model (especially Logistic Regression and KNN) perform better.
skewed_features = [
    'Horizontal_Distance_To_Hydrology',
    'Vertical_Distance_To_Hydrology',
    'Horizontal_Distance_To_Roadways',
    'Horizontal_Distance_To_Fire_Points'
]

for col in skewed_features:
    if col in df.columns:
        df[col] = np.log1p(df[col])

print("Skewness transformation complete.")

# 3. Detect and Handle Outliers using IQR Method
# We will cap the outliers to the upper and lower bounds to prevent data loss
# while maintaining the statistical integrity of the features.

def handle_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Capping the outliers
    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])
    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])
    return df

# Apply the outlier handling to key continuous variables
continuous_vars = ['Elevation', 'Aspect', 'Slope', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']
for var in continuous_vars:
    if var in df.columns:
        df = handle_outliers_iqr(df, var)

print("Outlier handling (capping) complete.")

"""## 4Ô∏è‚É£ Feature Engineering"""

import pandas as pd
import numpy as np
import pickle
from sklearn.preprocessing import LabelEncoder

# 1. Interaction Terms & Derived Features
# Creating new features based on geographical logic to improve model interpretability
# Distance to Hydrology (combining Horizontal and Vertical into a direct distance)
df['Euclidean_Distance_To_Hydrology'] = np.sqrt(
    df['Horizontal_Distance_To_Hydrology']**2 +
    df['Vertical_Distance_To_Hydrology']**2
)

# Shade Difference Index (difference between 9am and 3pm shades)
df['Shade_Difference_9am_3pm'] = df['Hillshade_9am'] - df['Hillshade_3pm']

# Elevation vs Roadways (interaction between height and infrastructure access)
df['Elevation_Roadway_Interaction'] = df['Elevation'] * df['Horizontal_Distance_To_Roadways']

print("Derived features created. New shape:", df.shape)

# 2. Encoding the Categorical Columns
# The 'Cover_Type' is our target with 7 classes. We must encode it from 0-6.
le = LabelEncoder()
df['Cover_Type'] = le.fit_transform(df['Cover_Type'])

# 3. Saving the Encoder for Inference
# This is a critical requirement from your project description.
# We save it now so that your Streamlit app can translate the 0-6 prediction
# back into the original Forest Cover Name (e.g., Spruce/Fir).
with open('label_encoder.pkl', 'wb') as f:
    pickle.dump(le, f)

print("Label Encoder saved successfully as 'label_encoder.pkl'.")

# 4. Final Feature Selection
# Ensure there are no non-numeric columns remaining before resampling/training
# (The dataset usually has binary indicators for Wilderness and Soil, which are already numeric)
print("\nFinal Data Types Check:")
print(df.dtypes.value_counts())

"""## 5Ô∏è‚É£ Exploratory Data Analysis (EDA)"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
import numpy as np # Import numpy for np.inf, np.nan
from sklearn.impute import SimpleImputer # Import SimpleImputer

# Set plot style
sns.set(style="whitegrid")

# 1. Class Distribution (Univariate) - Visualizing Imbalance
plt.figure(figsize=(8, 5))
sns.countplot(x='Cover_Type', data=df, palette='viridis')
plt.title('Plot 1: Distribution of Forest Cover Types (Class Imbalance)')
plt.show()

# 2. Elevation Distribution (Univariate) - Histogram
plt.figure(figsize=(8, 5))
sns.histplot(df['Elevation'], bins=30, kde=True, color='forestgreen')
plt.title('Plot 2: Univariate Analysis - Elevation Distribution')
plt.show()

# 3. Elevation vs Cover Type (Bivariate) - Boxplot
# Shows how different forest types exist at different altitudes
plt.figure(figsize=(10, 6))
sns.boxplot(x='Cover_Type', y='Elevation', data=df)
plt.title('Plot 3: Bivariate Analysis - Elevation by Cover Type')
plt.show()

# 4. Aspect vs Slope (Bivariate) - Scatter Plot
# Examining the relationship between terrain direction and steepness
plt.figure(figsize=(8, 5))
sns.scatterplot(x='Aspect', y='Slope', hue='Cover_Type', data=df.sample(2000), alpha=0.5)
plt.title('Plot 4: Aspect vs Slope (Sampled Data)')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

# 5. Correlation Heatmap
# Identifying multicollinearity and feature relationships
plt.figure(figsize=(12, 10))
# Selecting only a subset of continuous variables for clarity
continuous_cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',
                   'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',
                   'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Cover_Type']
sns.heatmap(df[continuous_cols].corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Plot 5: Correlation Heatmap of Continuous Features')
plt.show()

# 6. Hillshade Comparison (Bivariate) - Noon vs 3pm
plt.figure(figsize=(8, 5))
sns.kdeplot(data=df, x='Hillshade_Noon', y='Hillshade_3pm', fill=True, cmap='Greens')
plt.title('Plot 6: Density Plot - Hillshade Noon vs 3pm')
plt.show()

# 7. Horizontal vs Vertical Distance to Hydrology
plt.figure(figsize=(8, 5))
sns.regplot(x='Horizontal_Distance_To_Hydrology', y='Vertical_Distance_To_Hydrology',
            data=df.sample(1000), scatter_kws={'alpha':0.3}, line_kws={'color':'red'})
plt.title('Plot 7: Relationship - Horizontal vs Vertical Distance to Water')
plt.show()

# 8. Feature Importance Plot (Using a Baseline Model)
# Fitting a quick Random Forest to see which features matter most before final tuning
X_temp = df.drop('Cover_Type', axis=1)
y_temp = df['Cover_Type']

# Handle potential infinity or NaN values in X_temp before fitting the model.
# This addresses the "ValueError: Input X contains infinity or a value too large for dtype('float32')."
# 1. Replace any infinity values with NaN.
X_temp.replace([np.inf, -np.inf], np.nan, inplace=True)

# 2. Impute any remaining NaN values using SimpleImputer (median strategy is robust).
# Initialize a new imputer specifically for X_temp to ensure it's self-contained.
imputer_for_X_temp = SimpleImputer(strategy='median')
X_temp_cols = X_temp.columns
X_temp = pd.DataFrame(imputer_for_X_temp.fit_transform(X_temp), columns=X_temp_cols)

baseline_model = RandomForestClassifier(n_estimators=50, random_state=42)
baseline_model.fit(X_temp, y_temp)

importances = pd.Series(baseline_model.feature_importances_, index=X_temp.columns)
plt.figure(figsize=(10, 8))
importances.nlargest(10).plot(kind='barh', color='teal')
plt.title('Plot 8: Top 10 Important Features (Baseline Model)')
plt.gca().invert_yaxis()
plt.show()

"""## 6Ô∏è‚É£ Class Imbalance Handling (RandomOverSampler)"""

from imblearn.over_sampling import RandomOverSampler
import pandas as pd

# 1. Separate Features (X) and Target (y)
# X contains all your cartographic variables and derived features
# y contains the 'Cover_Type' (the 7 classes)
X = df.drop('Cover_Type', axis=1)
y = df['Cover_Type']

print(f"Original dataset shape: {X.shape}")
print("Original class distribution:")
print(y.value_counts().sort_index())

# 2. Initialize RandomOverSampler
# sampling_strategy='not majority' tells it to resample all classes except the largest one
ros = RandomOverSampler(sampling_strategy='not majority', random_state=42)

# 3. Fit and Resample
# This is where the row count increases to balance all 7 classes
X_resampled, y_resampled = ros.fit_resample(X, y)

# 4. Verification of Data Increase
print("\n" + "="*30)
print(f"Resampled dataset shape: {X_resampled.shape}")
print(f"Total rows added: {X_resampled.shape[0] - X.shape[0]}")
print("\nNew balanced class distribution (All 7 classes should be equal):")
print(pd.Series(y_resampled).value_counts().sort_index())
print("="*30)

# 5. Create a new balanced DataFrame for the next steps
df_balanced = pd.concat([pd.DataFrame(X_resampled), pd.Series(y_resampled, name='Cover_Type')], axis=1)

"""## 7Ô∏è‚É£ Feature Selection"""

from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.impute import SimpleImputer # Import SimpleImputer
import numpy as np # Import numpy for np.inf, np.nan

# 1. Initialize a Forest model for selection
# We use a smaller number of trees (n_estimators=100) to quickly calculate importance
selector_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)

# Handle potential infinity or NaN values in X_resampled before fitting the model.
# This addresses the "ValueError: Input X contains infinity or a value too large for dtype('float32')."
# 1. Replace any infinity values with NaN.
X_resampled.replace([np.inf, -np.inf], np.nan, inplace=True)

# 2. Impute any remaining NaN values using SimpleImputer (median strategy is robust).
imputer_for_resampled = SimpleImputer(strategy='median')
X_resampled_cols = X_resampled.columns
X_resampled = pd.DataFrame(imputer_for_resampled.fit_transform(X_resampled), columns=X_resampled_cols)

# 2. Fit the model on our balanced dataset
selector_model.fit(X_resampled, y_resampled)

# 3. Extract and Sort Feature Importances
importances = selector_model.feature_importances_
feature_names = X.columns
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# 4. Visualize Top Features
plt.figure(figsize=(12, 8))
# Plotting top 20 features for clarity
sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(20), palette='magma')
plt.title('Top 20 Most Influential Features for Forest Cover Prediction')
plt.show()

# 5. Drop Low-Importance Features
# Identify features that contribute almost nothing (e.g., importance < 0.001)
# This often includes certain Soil_Types that are rare across all classes
threshold = 0.001
low_importance_features = feature_importance_df[feature_importance_df['Importance'] < threshold]['Feature'].tolist()

print(f"Number of features to be dropped: {len(low_importance_features)}")
print(f"Dropped features: {low_importance_features}")

# Create the final feature set by dropping the identified columns
X_final = X_resampled.drop(columns=low_importance_features)

print(f"Final Feature Set Shape: {X_final.shape}")

"""## 8Ô∏è‚É£ Model Building and Evaluation"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Split the Resampled Data
# We use X_final (the feature-selected data) and y_resampled
X_train, X_test, y_train, y_test = train_test_split(X_final, y_resampled, test_size=0.2, random_state=42)

# 2. Define the 5 Required Models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
}

# 3. Training and Evaluation Loop
model_results = {}

for name, model in models.items():
    print(f"\n--- Training {name} ---")
    # Fit the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Calculate Metrics
    acc = accuracy_score(y_test, y_pred)
    model_results[name] = acc

    print(f"Accuracy: {acc:.4f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    # Plot Confusion Matrix
    plt.figure(figsize=(6, 4))
    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix: {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# 4. Compare and select the best performer
best_model_name = max(model_results, key=model_results.get)
print(f"\nüèÜ The best performing model is: {best_model_name}")

"""## üõ† Hyperparameter Tuning (RandomizedSearchCV)"""

from sklearn.model_selection import RandomizedSearchCV

# For this example, we assume Random Forest was the best.
# You can switch this to XGBoost if it performed better.
print(f"\n--- Optimizing {best_model_name} using RandomizedSearchCV ---")

# Define the parameter grid
# We search for the best number of trees, depth, and split criteria
param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Initialize RandomizedSearchCV
# n_iter=10 means it will try 10 random combinations from the grid
# cv=3 means 3-fold cross-validation
random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_distributions=param_dist,
    n_iter=10,
    cv=3,
    verbose=2,
    random_state=42,
    n_jobs=-1
)

# Fit the search
random_search.fit(X_train, y_train)

# Get the best estimator
final_model = random_search.best_estimator_

print("\nBest Parameters Found:")
print(random_search.best_params_)

# Final Evaluation of the tuned model
final_pred = final_model.predict(X_test)
print(f"\nFinal Tuned Accuracy: {accuracy_score(y_test, final_pred):.4f}")

# Save the final optimized model for the Streamlit App
import pickle
with open('best_forest_model.pkl', 'wb') as f:
    pickle.dump(final_model, f)

print("\nFinal model saved as 'best_forest_model.pkl'")

"""## 9Ô∏è‚É£ Finalize and Save Best Model"""

import pickle
import os

# 1. Selection of the final model
# 'final_model' is the best estimator returned from your RandomizedSearchCV
# 'final_model' should be the tuned version of Random Forest or XGBoost
print("Finalizing the model for deployment...")

# 2. Define the file names
model_filename = 'best_forest_model.pkl'
features_filename = 'model_features.pkl'

# 3. Save the trained model
# 'wb' stands for write-binary mode
with open(model_filename, 'wb') as model_file:
    pickle.dump(final_model, model_file)

# 4. Save the final list of columns (Feature Selection output)
# Your Streamlit app needs to know which features were kept after dropping
# the low-importance ones in Step 7.
with open(features_filename, 'wb') as features_file:
    pickle.dump(X_final.columns.tolist(), features_file)

print(f"‚úÖ Success! Model saved as: {model_filename}")
print(f"‚úÖ Success! Feature list saved as: {features_filename}")

# 5. Verification: Check the file size to ensure it saved correctly
file_size = os.path.getsize(model_filename) / (1024 * 1024) # Size in MB
print(f"Model File Size: {file_size:.2f} MB")

import pandas as pd
import numpy as np
import pickle
from imblearn.over_sampling import RandomOverSampler
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier

# 1. Load Data
df = pd.read_csv('cover_type.csv') # Ensure this file is in your folder

# 2. Data Cleaning & Encoding
le = LabelEncoder()
df['Cover_Type'] = le.fit_transform(df['Cover_Type'])

# 3. Class Imbalance (The 7x Increase)
X = df.drop('Cover_Type', axis=1)
y = df['Cover_Type']

ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)
print(f"Data increased to: {len(X_resampled)} rows")

# 4. RandomizedSearchCV (Optimizing the model)
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2)
rf = RandomForestClassifier()
param_dist = {'n_estimators': [100, 200], 'max_depth': [10, 20, None]}

search = RandomizedSearchCV(rf, param_dist, n_iter=5, cv=3)
search.fit(X_train, y_train)

# 5. SAVE THE FILES (The Rectification)
# These names must match exactly what the app.py looks for
with open('best_forest_model.pkl', 'wb') as f:
    pickle.dump(search.best_estimator_, f)

with open('label_encoder.pkl', 'wb') as f:
    pickle.dump(le, f)

# Save the list of feature names used during training
with open('model_features.pkl', 'wb') as f:
    pickle.dump(X.columns.tolist(), f)

print("Files saved successfully! Now you can run the Streamlit app.")

import pandas as pd
import numpy as np
import pickle
from imblearn.over_sampling import RandomOverSampler
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# 1. Load Data
# Make sure 'forest_cover_data.csv' is in the same folder
print("Step 1: Loading Dataset...")
df = pd.read_csv('cover_type.csv')

# 2. Data Cleaning & Encoding
print("Step 2: Cleaning and Encoding...")
# Encode the Target FIRST so it becomes numerical BEFORE any imputation that uses df.median()
le = LabelEncoder()
df['Cover_Type'] = le.fit_transform(df['Cover_Type'])

# Handle missing values quickly. Now all columns should be numerical for df.median() to work.
df = df.fillna(df.median())

# 3. Class Imbalance (The 7x Increase)
print("Step 3: Handling Class Imbalance (Oversampling)...")
X = df.drop('Cover_Type', axis=1)
y = df['Cover_Type']

# RandomOverSampler makes all 7 classes equal in size
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)
print(f"Dataset balanced. New row count: {len(X_resampled)}")

# 4. Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.2, random_state=42
)

# 5. RandomizedSearchCV (Optimized for Speed)
print("Step 4: Hyperparameter Tuning (This may take a few minutes)...")
rf = RandomForestClassifier(random_state=42)

# Smaller parameter grid for faster execution during evaluation
param_dist = {
    'n_estimators': [100, 150],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5]
}

# n_iter=3 reduces the number of combinations tested to save time
# n_jobs=-1 uses all CPU cores to speed up training
search = RandomizedSearchCV(
    rf, param_distributions=param_dist,
    n_iter=3, cv=2, n_jobs=-1, verbose=1
)

search.fit(X_train, y_train)
best_model = search.best_estimator_

# 6. Final Evaluation
y_pred = best_model.predict(X_test)
print(f"\nModel Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# 7. SAVE THE FILES (Crucial for Streamlit)
print("\nStep 5: Saving .pkl files...")
with open('best_forest_model.pkl', 'wb') as f:
    pickle.dump(best_model, f)

with open('label_encoder.pkl', 'wb') as f:
    pickle.dump(le, f)

with open('model_features.pkl', 'wb') as f:
    pickle.dump(X.columns.tolist(), f)

print("All steps complete! You can now run 'streamlit run app.py'")

